import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
# ê°œë³„ íŒŒì¼ ë¡œë”ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤. UnstructuredPowerPointLoader, UnstructuredFileLoader í¬í•¨
# PyPDFLoader ëŒ€ì‹  UnstructuredFileLoaderë¥¼ PDFì— ì‚¬ìš©í•©ë‹ˆë‹¤.
from langchain_community.document_loaders import Docx2txtLoader, TextLoader, UnstructuredPowerPointLoader, UnstructuredFileLoader 
# RecursiveCharacterTextSplitterë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
from langchain.text_splitter import RecursiveCharacterTextSplitter  # ì˜¤íƒ€ ìˆ˜ì •
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# AIMessage ì„í¬íŠ¸ ì¶”ê°€
from langchain_core.messages import AIMessage
import nltk 

# â˜…â˜…â˜… st.set_page_config()ë¥¼ main() í•¨ìˆ˜ ë°–ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ë„ë¡ í•©ë‹ˆë‹¤. â˜…â˜…â˜…
st.set_page_config(
    page_title="RAG ë¬¸ì„œ Q&A ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# OpenAI API Key ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ====================================
# PRE-PROCESSING ë‹¨ê³„
# ====================================

class DocumentProcessor:
    """ë¬¸ì„œ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    @staticmethod
    @st.cache_resource
    def load_documents(directory_path: str):
        # ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. initialize_rag_systemì„ í™•ì¸í•˜ì„¸ìš”.
        return []

    @staticmethod
    def split_text(documents, chunk_size=50, chunk_overlap=10):
        """
        2. í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í¬ ë¶„í• )
        - ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        - RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸€ì ë‹¨ìœ„ë¡œ ë¶„í• í•˜ë©°, OpenAI í† í° ì œí•œì„ ìœ„í•´ chunk_sizeë¥¼ ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        text_splitter = RecursiveCharacterTextSplitter(  # ì˜¬ë°”ë¥¸ í´ë˜ìŠ¤ëª…
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""] 
        )
        split_docs = text_splitter.split_documents(documents)
        return split_docs

    @staticmethod
    # @st.cache_resource 
    def create_vector_store(_split_docs, embeddings): 
        """
        4. DB ì €ì¥ (ë²¡í„° ì €ì¥ì†Œ)
        - ë³€í™˜ëœ ë²¡í„°ë¥¼ FAISS DBì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        vectorstore = FAISS.from_documents(_split_docs, embeddings)
        return vectorstore
    
    @staticmethod
    def add_documents_to_vector_store(vectorstore, split_docs, embeddings): 
        """
        ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œì— ìƒˆë¡œìš´ ë¬¸ì„œ ì²­í¬ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        vectorstore.add_documents(split_docs) 
        return vectorstore

# ====================================
# RUNTIME ë‹¨ê³„
# ====================================

class RAGRetriever:
    """ê²€ìƒ‰ê¸°(Retriever) ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, vectorstore):
        self.vectorstore = vectorstore

    def get_retriever(self, search_type="similarity", k=5):
        """
        1. ê²€ìƒ‰ (Retrieve)
        - ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (kê°’ì„ 5ë¡œ ì¡°ì •í•˜ì—¬ ë” ë§ì€ ë¬¸ë§¥ ì°¸ì¡°)
        """
        retriever = self.vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
        return retriever

class PromptManager:
    """í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""

    @staticmethod
    def get_contextualize_prompt():
        """
        2. í”„ë¡¬í”„íŠ¸ (í”„ë¡¬í”„íŠ¸) - ëŒ€í™” ë§¥ë½í™”
        - ì±„íŒ… ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        contextualize_q_system_prompt = """ì£¼ì–´ì§„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ,
        ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

        return ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ])

    @staticmethod
    def get_qa_prompt():
        """
        2. í”„ë¡¬í”„íŠ¸ (í”„ë¡¬í”„íŠ¸) - ì§ˆë¬¸ ë‹µë³€
        - ê²€ìƒ‰ëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        """
        qa_system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

        ë‹µë³€ ê·œì¹™:
        - ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.
        - ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” 'ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        - ê°€ëŠ¥í•˜ë‹¤ë©´ ê´€ë ¨ëœ ë¬¸ì„œì˜ ì¶œì²˜(source)ì™€ í˜ì´ì§€(page)ë¥¼ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ëœ ë¬¸ë§¥:
        {context}"""

        return ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ])
    
    @staticmethod
    def get_general_qa_prompt():
        """
        ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        """
        general_system_prompt = """ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ì–´ë–¤ ìƒí™©ì—ì„œë„ ë¬¸ì„œ ê²€ìƒ‰ì„ ì‹œë„í•˜ì§€ ë§ê³ , ì˜¤ì§ ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""
        return ChatPromptTemplate.from_messages([
            ("system", general_system_prompt),
            MessagesPlaceholder("history"), 
            ("human", "{input}"),
        ])

    @staticmethod
    def get_intent_detection_prompt(): 
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. LLMì´ 'DOCUMENTS' ë˜ëŠ” 'GENERAL' í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°˜í™˜í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
        """
        return ChatPromptTemplate.from_messages([
            ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ 'DOCUMENTS', ì¼ë°˜ì ì¸ ì§€ì‹ ì§ˆë¬¸ì´ë©´ 'GENERAL'. ë‹µë³€ì€ ì˜¤ì§ 'DOCUMENTS' ë˜ëŠ” 'GENERAL' ì¤‘ í•˜ë‚˜ë¡œë§Œ í•˜ì„¸ìš”."),
            ("human", "{question}"), 
        ])


class LLMManager:
    """
    3. ì–¸ì–´ ëª¨ë¸ (LLM) ê´€ë¦¬
    - GPT-4o-mini ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    def __init__(self, model_name="gpt-4o-mini"):
        self.model_name = model_name
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
        )

    def get_llm(self):
        return self.llm

class RAGChain:
    """RAG ì²´ì¸ êµ¬ì„± ë° ê´€ë¦¬"""

    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
        self.chain = self._build_chain()

    def _build_chain(self):
        prompt_manager = PromptManager()
        history_aware_retriever = create_history_aware_retriever(
            self.llm, self.retriever, prompt_manager.get_contextualize_prompt()
        )
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt_manager.get_qa_prompt())
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        return rag_chain

    def get_conversational_chain(self, chat_history):
        # output_messages_key="answer"ë¥¼ ì„¤ì •í•˜ë©´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€
        return RunnableWithMessageHistory(
            self.chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer", 
        )

# ====================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ====================================

@st.cache_resource 
def download_nltk_data():
    nltk_data_path = os.path.join(os.getcwd(), "nltk_data")
    if not os.path.exists(nltk_data_path):
        os.makedirs(nltk_data_path)
    nltk.data.path.append(nltk_data_path)  # .path ì†ì„±ì— append

    datasets = ['punkt', 'averaged_perceptron_tagger']

    for dataset in datasets:
        try:
            if dataset == 'punkt':
                nltk.data.find(f'tokenizers/{dataset}')
            else: 
                nltk.data.find(f'taggers/{dataset}')
        except LookupError: 
            try:
                nltk.download(dataset, quiet=True, download_dir=nltk_data_path)
            except Exception as e_download: 
                st.error(f"NLTK '{dataset}' ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e_download}") 
                return None 
        except Exception as e_other: 
            st.error(f"NLTK '{dataset}' ë°ì´í„° í™•ì¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_other}") 
            return None 
    return True 


def initialize_rag_system(model_name):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê°œë³„ ë¬¸ì„œ ì²˜ë¦¬ ë°©ì‹)"""
    data_path = "./data"  # ë¬¸ì„œ í´ë” ê²½ë¡œ
    vectorstore = None  # ì´ˆê¸° ë²¡í„° ì €ì¥ì†ŒëŠ” Noneìœ¼ë¡œ ì„¤ì •
    
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small') 
    
    general_llm_manager = LLMManager(model_name)
    general_llm = general_llm_manager.get_llm()

    processed_any_document = False
    for filename in os.listdir(data_path):
        filepath = os.path.join(data_path, filename)
        
        if os.path.isfile(filepath): 
            try:
                if filename.lower().endswith(".pdf"):
                    loader = UnstructuredFileLoader(filepath) 
                elif filename.lower().endswith(".docx"):
                    loader = Docx2txtLoader(filepath)
                elif filename.lower().endswith(".pptx"):
                    loader = UnstructuredPowerPointLoader(filepath) 
                elif filename.lower().endswith(".txt"):
                    loader = TextLoader(filepath, encoding="utf-8") 
                # CSVLoaderëŠ” UnstructuredFileLoaderê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.
                # elif filename.lower().endswith(".csv"): 
                #     loader = CSVLoader(filepath)
                else:
                    continue 

                single_document_list = loader.load() 
                
                if not single_document_list:
                    continue

                split_single_doc_chunks = DocumentProcessor.split_text(single_document_list)
                
                if vectorstore is None:
                    vectorstore = DocumentProcessor.create_vector_store(split_single_doc_chunks, embeddings)
                else:
                    vectorstore = DocumentProcessor.add_documents_to_vector_store(vectorstore, split_single_doc_chunks, embeddings)
                
                processed_any_document = True

            except Exception as e:
                st.error(f"âŒ íŒŒì¼ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") 
                continue 

    if not processed_any_document or vectorstore is None:
        st.error("âŒ 'data' í´ë”ì— ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.") 
        return None 
    
    rag_retriever = RAGRetriever(vectorstore)
    retriever = rag_retriever.get_retriever()
    llm_manager = LLMManager(model_name)
    llm = llm_manager.get_llm()
    rag_chain = RAGChain(retriever, llm)  # RAG ì²´ì¸
    
    return rag_chain, general_llm 


def format_output(response):
    """
    ì²´ì¸ ì‘ë‹µì„ í†µì¼ëœ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    responseëŠ” dict ë˜ëŠ” AIMessage ê°ì²´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    answer = 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
    context = []
    source_count = 0

    if isinstance(response, dict):
        answer = response.get('answer', answer)
        context = response.get('context', context)
        source_count = len(context)
    elif isinstance(response, AIMessage): 
        answer = response.content 
        context = [] 
        source_count = 0
    # ë‹¤ë¥¸ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì¼ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)
    else: 
        answer = str(response)

    return {
        'answer': answer,
        'context': context,
        'source_count': source_count
    }

# ====================================
# Streamlit UI
# ====================================

def main():
    # Session State ì´ˆê¸°í™”
    if 'force_rag_keywords' not in st.session_state:
        st.session_state.force_rag_keywords = ""
    # ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸ ìœ í˜• ì„¤ì • ìƒíƒœ ì´ˆê¸°í™”
    if 'question_type_override' not in st.session_state:
        st.session_state.question_type_override = "ìë™ ë¶„ë¥˜"  # ê¸°ë³¸ê°’

    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    nltk_download_status = download_nltk_data()
    if nltk_download_status is None: 
        return

    st.header("ğŸ¤– RAG ê¸°ë°˜ ë¬¸ì„œ Q&A ì±—ë´‡ ğŸ’¬")
    st.markdown("`data` í´ë”ì˜ ë¬¸ì„œ(PDF, TXT, DOCX ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

    with st.sidebar:
        st.header("ğŸ”§ ì„¤ì •")
        model_option = st.selectbox(
            "GPT ëª¨ë¸ ì„ íƒ",
            ("gpt-4o-mini", "gpt-3.5-turbo-0125", "gpt-4o"),
            help="ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        st.markdown("---")
        
        # RAG ê°•ì œ í™œì„±í™” í‚¤ì›Œë“œ ì…ë ¥ í•„ë“œ
        st.session_state.force_rag_keywords = st.text_input(
            "RAG ê°•ì œ í™œì„±í™” í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            value=st.session_state.force_rag_keywords,
            help="ì—¬ê¸°ì— ì…ë ¥ëœ í‚¤ì›Œë“œê°€ ì§ˆë¬¸ì— í¬í•¨ë˜ë©´, LLM ì˜ë„ ë¶„ë¥˜ì™€ ê´€ê³„ì—†ì´ RAG ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰ë©ë‹ˆë‹¤. (ì˜ˆ: ë¬¸ì„œ, ë³´ê³ ì„œ, íŒŒì¼)"
        )
        st.markdown("---")  # êµ¬ë¶„ì„  ì¶”ê°€

        # ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸ ìœ í˜• ì„ íƒ í•„ë“œ ì¶”ê°€
        st.session_state.question_type_override = st.radio(
            "ì§ˆë¬¸ ìœ í˜• ì„¤ì •",
            ("ìë™ ë¶„ë¥˜", "ë¬¸ì„œ ê´€ë ¨ ê°•ì œ", "ì¼ë°˜ ì§€ì‹ ê°•ì œ"),
            index=["ìë™ ë¶„ë¥˜", "ë¬¸ì„œ ê´€ë ¨ ê°•ì œ", "ì¼ë°˜ ì§€ì‹ ê°•ì œ"].index(st.session_state.question_type_override),
            help="ì§ˆë¬¸ ì²˜ë¦¬ ë°©ì‹ì„ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. 'ìë™ ë¶„ë¥˜'ëŠ” LLM ë˜ëŠ” í‚¤ì›Œë“œì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤."
        )
        st.markdown("---")  # êµ¬ë¶„ì„  ì¶”ê°€

        st.info("`data` í´ë”ì— íŒŒì¼ì„ ì¶”ê°€/ì‚­ì œí•œ í›„ì—ëŠ” í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        st.markdown("---")
        st.markdown("### ğŸ“Š RAG í”„ë¡œì„¸ìŠ¤")
        st.markdown("""
        **Pre-processing:**
        1. ğŸ“„ ë¬¸ì„œ ë¡œë“œ (ê°œë³„ íŒŒì¼ ì²˜ë¦¬)
        2. âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  (ë§¤ìš° ì‘ì€ ì²­í¬)
        3. ğŸ’¾ ë²¡í„° DB ì €ì¥/ì¶”ê°€ (ê° ë¬¸ì„œ ì²­í¬ë³„)

        **Runtime (ìë™ ë¼ìš°íŒ…):**
        1. ğŸ¤” ì§ˆë¬¸ ì˜ë„ ê°ì§€ (ë¬¸ì„œ ê´€ë ¨ vs ì¼ë°˜ ì§€ì‹)
        2. ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ (í•„ìš”ì‹œ)
        3. ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        4. ğŸ¤– LLM ì¶”ë¡ 
        5. ğŸ“‹ ê²°ê³¼ ì¶œë ¥
        """)

    rag_chain_wrapper, llm_for_general_qa = initialize_rag_system(model_option) 
    
    if rag_chain_wrapper is None: 
        return

    chat_history = StreamlitChatMessageHistory(key="chat_messages")
    conversational_rag_chain = rag_chain_wrapper.get_conversational_chain(chat_history)
    
    prompt_manager = PromptManager() 
    general_llm_chain_template = prompt_manager.get_general_qa_prompt() 
    
    general_qa_chain_raw = general_llm_chain_template | llm_for_general_qa  # LCEL ì‚¬ìš©
    general_conversational_chain = RunnableWithMessageHistory(
        general_qa_chain_raw, 
        lambda session_id: chat_history, 
        input_messages_key="input",
        history_messages_key="history",
        output_messages_key="answer", 
    )
    
    # --- ì˜ë„ ê°ì§€ ì²´ì¸ ìƒì„± (í…ìŠ¤íŠ¸ ë°˜í™˜ ë° íŒŒì‹±ìœ¼ë¡œ ë³€ê²½) ---
    intent_detection_prompt = prompt_manager.get_intent_detection_prompt()
    intent_detection_llm = ChatOpenAI(model=model_option, temperature=0)  # temperature=0ìœ¼ë¡œ ì¼ê´€ëœ ë¶„ë¥˜ ìœ ë„
    
    # í”„ë¡¬í”„íŠ¸ì™€ LLMì„ ì§ì ‘ ì—°ê²° (structured_output ì‚¬ìš© ì œê±°)
    intent_detection_chain_pre_invoke = intent_detection_prompt | intent_detection_llm
    # --------------------------------------------------------
    
    if not chat_history.messages:
        chat_history.add_ai_message("ì•ˆë…•í•˜ì„¸ìš”! `data` í´ë”ì˜ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ“š")

    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
        st.chat_message("human").write(prompt)

        with st.chat_message("ai"):
            with st.spinner("ğŸ§ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."): 
                try:
                    # â˜…â˜…â˜… ì§ˆë¬¸ ìœ í˜• ê²°ì • ë¡œì§ ìˆ˜ì • â˜…â˜…â˜…
                    determined_intent = ""
                    
                    # 1. ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì • ìš°ì„  ì ìš©
                    if st.session_state.question_type_override == "ë¬¸ì„œ ê´€ë ¨ ê°•ì œ":
                        determined_intent = "DOCUMENTS"
                        st.info("âš™ï¸ ì‚¬ìš©ì ì„¤ì •ì— ë”°ë¼ RAG ëª¨ë“œë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                    elif st.session_state.question_type_override == "ì¼ë°˜ ì§€ì‹ ê°•ì œ":
                        determined_intent = "GENERAL"
                        st.info("âš™ï¸ ì‚¬ìš©ì ì„¤ì •ì— ë”°ë¼ ì¼ë°˜ LLM ëª¨ë“œë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                    else:  # "ìë™ ë¶„ë¥˜"ì¼ ê²½ìš°
                        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ RAG ê°•ì œ í™œì„±í™” ë¡œì§
                        force_rag_by_keyword = False
                        if st.session_state.force_rag_keywords:
                            keywords = [k.strip().lower() for k in st.session_state.force_rag_keywords.split(',') if k.strip()]
                            for keyword in keywords:
                                if keyword in prompt.lower():
                                    force_rag_by_keyword = True
                                    break
                        
                        if force_rag_by_keyword:
                            determined_intent = "DOCUMENTS"  # í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ë©´ RAG ê°•ì œ
                            st.info(f"ğŸ”‘ í‚¤ì›Œë“œ '{', '.join(keywords)}' ê°ì§€! RAG ëª¨ë“œë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                        else:
                            # 3. LLM ê¸°ë°˜ ì˜ë„ ê°ì§€
                            try:
                                intent_response_message = intent_detection_chain_pre_invoke.invoke(
                                    {"question": prompt} 
                                )
                                determined_intent = intent_response_message.content.strip().upper() 
                            except Exception as e:
                                st.warning(f"LLM ì˜ë„ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ì ìœ¼ë¡œ RAG ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                                determined_intent = "DOCUMENTS"  # LLM ì˜ë„ ê°ì§€ ì‹¤íŒ¨ ì‹œ RAG í´ë°±

                    final_answer = ""
                    final_context = []
                    final_source_count = 0
                    used_rag_successfully = False 

                    if determined_intent == "GENERAL": 
                        st.info("ğŸ’¡ ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
                        config = {"configurable": {"session_id": "general_chat"}}
                        response_from_llm_formatted = format_output(general_conversational_chain.invoke({"input": prompt}, config))
                        final_answer = response_from_llm_formatted['answer']

                    elif determined_intent == "DOCUMENTS":
                        st.info("ğŸ” ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‹µë³€í•©ë‹ˆë‹¤.")
                        config = {"configurable": {"session_id": "rag_chat"}}
                        response_from_rag_formatted = format_output(conversational_rag_chain.invoke({"input": prompt}, config))
                        
                        rag_answer_content = response_from_rag_formatted['answer']

                        # LLMì´ "ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ í´ë°±
                        if "ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." in rag_answer_content or not response_from_rag_formatted.get('context'):
                            st.warning("âš ï¸ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•˜ì—¬ LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                            config = {"configurable": {"session_id": "general_chat"}} 
                            response_from_llm_formatted = format_output(general_conversational_chain.invoke({"input": prompt}, config))
                            final_answer = response_from_llm_formatted['answer']
                        else:
                            final_answer = response_from_rag_formatted['answer']
                            final_context = response_from_rag_formatted['context']
                            final_source_count = response_from_rag_formatted['source_count']
                            used_rag_successfully = True 

                    else:  # ì˜ë„ íŒŒì•… ì‹¤íŒ¨ ì‹œ (LLMì´ 'DOCUMENTS' ë˜ëŠ” 'GENERAL' ì™¸ì˜ ê²ƒì„ ë°˜í™˜í•œ ê²½ìš°) ê¸°ë³¸ RAG ëª¨ë“œë¡œ ì§„í–‰
                        st.warning(f"ì˜ë„ íŒŒì•…ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì‘ë‹µ: {determined_intent}). ê¸°ë³¸ì ìœ¼ë¡œ RAG ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                        config = {"configurable": {"session_id": "rag_chat"}}
                        response_from_rag_formatted = format_output(conversational_rag_chain.invoke({"input": prompt}, config))
                        
                        rag_answer_content = response_from_rag_formatted['answer']
                        if "ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." in rag_answer_content or not response_from_rag_formatted.get('context'):
                            st.warning("âš ï¸ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•˜ì—¬ LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤. (ì˜ë„ íŒŒì•… ì‹¤íŒ¨ í›„)")
                            config = {"configurable": {"session_id": "general_chat"}} 
                            response_from_llm_formatted = format_output(general_conversational_chain.invoke({"input": prompt}, config))
                            final_answer = response_from_llm_formatted['answer']
                        else:
                            final_answer = response_from_rag_formatted['answer']
                            final_context = response_from_rag_formatted['context']
                            final_source_count = response_from_rag_formatted['source_count']
                            used_rag_successfully = True

                    st.write(final_answer)

                    if used_rag_successfully:
                        with st.expander(f"ğŸ“„ ì°¸ê³  ë¬¸ì„œ ({final_source_count}ê°œ)"):
                            if final_context: 
                                for i, doc in enumerate(final_context):
                                    st.markdown(f"**ğŸ“– ë¬¸ì„œ {i+1}**")
                                    source = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                                    st.markdown(f"**ì¶œì²˜:** `{source}`")
                                    if 'page' in doc.metadata:
                                        page = doc.metadata.get('page')
                                        st.markdown(f"**í˜ì´ì§€:** {page + 1}")
                                    st.text_area(
                                        f"ë¬¸ì„œ {i+1} ë‚´ìš©",
                                        doc.page_content,
                                        height=150,
                                        key=f"doc_{i}",
                                        label_visibility="collapsed"
                                    )
                                    if i < len(final_context) - 1:
                                        st.markdown("---")
                            else: 
                                st.info("ë‹µë³€ì— ì°¸ê³ í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") 
                    else: 
                        st.info("ë‹µë³€ì— ì°¸ê³ í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (ì¼ë°˜ LLM ë‹µë³€)") 

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.info("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
