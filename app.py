import os
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_community.document_loaders import DirectoryLoader # DirectoryLoaderë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import nltk
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter


# OpenAI API Key ì„¤ì •
# Streamlitì˜ secretsì— 'OPENAI_API_KEY'ë¥¼ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# ====================================
# PRE-PROCESSING ë‹¨ê³„
# ====================================

class DocumentProcessor:
    """ë¬¸ì„œ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    @staticmethod
    @st.cache_resource
    def load_documents(directory_path: str):
        """
        1. ë¬¸ì„œ ë¡œë“œ (Document Load)
        - ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë“  í˜•ì‹ì˜ íŒŒì¼(.pdf, .txt, .docx ë“±)ì„ ì½ì–´ë“¤ì…ë‹ˆë‹¤.
        """
        if not os.path.isdir(directory_path):
            st.error(f"ì˜¤ë¥˜: '{directory_path}' ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # DirectoryLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ë¡œë“œ
        # show_progress=Trueë¡œ ì„¤ì •í•˜ì—¬ ë¡œë”© ì§„í–‰ ìƒí™©ì„ í„°ë¯¸ë„ì— í‘œì‹œí•©ë‹ˆë‹¤.
        # use_multithreading=Trueë¡œ ì„¤ì •í•˜ì—¬ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ë¹ ë¥´ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
        loader = DirectoryLoader(directory_path, glob="**/*.*", show_progress=True, use_multithreading=True)
        
        st.info(f"ğŸ“ '{directory_path}' ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        try:
            documents = loader.load()
        except Exception as e:
            st.error(f"ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            documents = [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”

        if not documents:
            st.error("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'data' ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        st.info(f"ğŸ“„ ì´ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)} ê°œì˜ ë¬¸ì„œ")
        return documents

    @staticmethod
    def split_text(documents, chunk_size=1000, chunk_overlap=200):
        """
        2. Text Split (ì²­í¬ ë¶„í• )
        - ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            encoding_name="cl100k_base"
        )
        split_docs = text_splitter.split_documents(documents)
        st.info(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(split_docs)} ì²­í¬")
        return split_docs

    @staticmethod
    @st.cache_resource
    def create_vector_store(_split_docs):
        """
        4. DB ì €ì¥ (Vector Store)
        - ë³€í™˜ëœ ë²¡í„°ë¥¼ FAISS DBì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
        vectorstore = FAISS.from_documents(_split_docs, embeddings)
        st.success("ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")
        return vectorstore

# ====================================
# RUNTIME ë‹¨ê³„
# ====================================

class RAGRetriever:
    """ê²€ìƒ‰ê¸°(Retriever) ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, vectorstore):
        self.vectorstore = vectorstore

    def get_retriever(self, search_type="similarity", k=5):
        """
        1. ê²€ìƒ‰ (Retrieve)
        - Vector DBì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (kê°’ì„ 5ë¡œ ì¡°ì •í•˜ì—¬ ë” ë§ì€ ë¬¸ë§¥ ì°¸ì¡°)
        """
        retriever = self.vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
        return retriever

class PromptManager:
    """í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""

    @staticmethod
    def get_contextualize_prompt():
        """
        2. í”„ë¡¬í”„íŠ¸ (Prompt) - ëŒ€í™” ë§¥ë½í™”
        - ì±„íŒ… ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        contextualize_q_system_prompt = """ì£¼ì–´ì§„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ,
        ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

        return ChatPromptTemplate.from_messages([
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ])

    @staticmethod
    def get_qa_prompt():
        """
        2. í”„ë¡¬í”„íŠ¸ (Prompt) - ì§ˆë¬¸ ë‹µë³€
        - ê²€ìƒ‰ëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
        """
        # "í—Œë²• ì „ë¬¸ê°€"ì—ì„œ "AI ì–´ì‹œìŠ¤í„´íŠ¸"ë¡œ ì¢€ ë” ì¼ë°˜ì ì¸ ì—­í• ë¡œ ë³€ê²½
        qa_system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

        ë‹µë³€ ê·œì¹™:
        - ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”.
        - ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” 'ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        - ê°€ëŠ¥í•˜ë‹¤ë©´ ê´€ë ¨ëœ ë¬¸ì„œì˜ ì¶œì²˜(source)ì™€ í˜ì´ì§€(page)ë¥¼ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ëœ ë¬¸ë§¥:
        {context}"""

        return ChatPromptTemplate.from_messages([
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ])

class LLMManager:
    """
    3. ì–¸ì–´ ëª¨ë¸ (LLM) ê´€ë¦¬
    - GPT-4o-mini ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    def __init__(self, model_name="gpt-4o-mini"):
        self.model_name = model_name
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
        )

    def get_llm(self):
        return self.llm

class RAGChain:
    """RAG ì²´ì¸ êµ¬ì„± ë° ê´€ë¦¬"""

    def __init__(self, retriever, llm):
        self.retriever = retriever
        self.llm = llm
        self.chain = self._build_chain()

    def _build_chain(self):
        prompt_manager = PromptManager()
        history_aware_retriever = create_history_aware_retriever(
            self.llm, self.retriever, prompt_manager.get_contextualize_prompt()
        )
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt_manager.get_qa_prompt())
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        return rag_chain

    def get_conversational_chain(self, chat_history):
        return RunnableWithMessageHistory(
            self.chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )

# ====================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ====================================

@st.cache_resource

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_nltk_data():
    st.info("NLTK ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")

    # NLTK ë°ì´í„°ê°€ ì €ì¥ë  ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
    # Streamlit Cloud í™˜ê²½ì—ì„œ ì“°ê¸° ê°€ëŠ¥í•œ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤.
    # ì¼ë°˜ì ìœ¼ë¡œ ì•±ì˜ ì‘ì—… ë””ë ‰í† ë¦¬ ë‚´ì— í´ë”ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
    nltk_data_path = os.path.join(os.getcwd(), "nltk_data")
    if not os.path.exists(nltk_data_path):
        os.makedirs(nltk_data_path)
    # NLTKê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ê²½ë¡œ ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    nltk.data.path.append(nltk_data_path)

    # ë‹¤ìš´ë¡œë“œí•  NLTK ë°ì´í„°ì…‹ ëª©ë¡
    datasets = ['punkt', 'averaged_perceptron_tagger']

    for dataset in datasets:
        try:
            # ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            if dataset == 'punkt':
                nltk.data.find(f'tokenizers/{dataset}')
            else: # averaged_perceptron_tagger ê°™ì€ íƒœê±°ëŠ” taggers í´ë”ì— ìˆìŠµë‹ˆë‹¤.
                nltk.data.find(f'taggers/{dataset}')
            st.success(f"âœ… NLTK '{dataset}' ë°ì´í„° í™•ì¸ ì™„ë£Œ!")
        except LookupError: # NLTK ë°ì´í„°ê°€ ì—†ì„ ë•Œ nltk.data.find()ê°€ ë°œìƒì‹œí‚¤ëŠ” ì¼ë°˜ì ì¸ ì˜ˆì™¸
            st.warning(f"NLTK '{dataset}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
            try:
                # ë°ì´í„°ë¥¼ ì§€ì •ëœ ê²½ë¡œì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (quiet=Trueë¡œ ë¶ˆí•„ìš”í•œ ì¶œë ¥ ì œê±°)
                nltk.download(dataset, quiet=True, download_dir=nltk_data_path)
                st.success(f"âœ… NLTK '{dataset}' ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
            except Exception as e_download: # ë‹¤ìš´ë¡œë“œ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì˜¤ë¥˜ë¥¼ ì¡ìŠµë‹ˆë‹¤.
                st.error(f"NLTK '{dataset}' ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {e_download}")
                st.stop() # í•„ìˆ˜ ë°ì´í„°ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì•±ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.
        except Exception as e_other: # LookupError ì™¸ì˜ ë‹¤ë¥¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¥¼ ì¡ìŠµë‹ˆë‹¤.
            st.error(f"NLTK '{dataset}' ë°ì´í„° í™•ì¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e_other}")
            st.stop() # í•„ìˆ˜ ë°ì´í„°ì´ë¯€ë¡œ ì‹¤íŒ¨ ì‹œ ì•±ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.

def initialize_rag_system(model_name):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    st.info("ğŸ”„ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    documents = DocumentProcessor.load_documents("data")
    split_docs = DocumentProcessor.split_text(documents)
    vectorstore = DocumentProcessor.create_vector_store(split_docs)
    rag_retriever = RAGRetriever(vectorstore)
    retriever = rag_retriever.get_retriever()
    llm_manager = LLMManager(model_name)
    llm = llm_manager.get_llm()
    rag_chain = RAGChain(retriever, llm)
    st.success("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return rag_chain

def format_output(response):
    """ê²°ê³¼ í¬ë§·íŒ…"""
    answer = response.get('answer', 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    context = response.get('context', [])
    return {
        'answer': answer,
        'context': context,
        'source_count': len(context)
    }

# ====================================
# Streamlit UI
# ====================================

def main():

    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    download_nltk_data()

    # í˜ì´ì§€ ì œëª©ê³¼ ì•„ì´ì½˜ì„ ì¢€ ë” ì¼ë°˜ì ì¸ ë‚´ìš©ìœ¼ë¡œ ë³€ê²½
    st.set_page_config(
        page_title="RAG ë¬¸ì„œ Q&A ì±—ë´‡",
        page_icon="ğŸ¤–",
        layout="wide"
    )

    st.header("ğŸ¤– RAG ê¸°ë°˜ ë¬¸ì„œ Q&A ì±—ë´‡ ğŸ’¬")
    st.markdown("`data` í´ë”ì˜ ë¬¸ì„œ(PDF, TXT, DOCX ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")

    with st.sidebar:
        st.header("ğŸ”§ ì„¤ì •")
        model_option = st.selectbox(
            "GPT ëª¨ë¸ ì„ íƒ",
            ("gpt-4o-mini", "gpt-3.5-turbo-0125", "gpt-4o"),
            help="ì‚¬ìš©í•  GPT ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        st.markdown("---")
        st.info("`data` í´ë”ì— íŒŒì¼ì„ ì¶”ê°€/ì‚­ì œí•œ í›„ì—ëŠ” í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        st.markdown("---")
        st.markdown("### ğŸ“Š RAG í”„ë¡œì„¸ìŠ¤")
        st.markdown("""
        **Pre-processing:**
        1. ğŸ“„ ë¬¸ì„œ ë¡œë“œ
        2. âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í• 
        3. ğŸ’¾ ë²¡í„° DB ì €ì¥

        **Runtime:**
        1. ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰
        2. ğŸ“ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        3. ğŸ¤– LLM ì¶”ë¡ 
        4. ğŸ“‹ ê²°ê³¼ ì¶œë ¥
        """)

    rag_chain = initialize_rag_system(model_option)
    chat_history = StreamlitChatMessageHistory(key="chat_messages")
    conversational_rag_chain = rag_chain.get_conversational_chain(chat_history)

    # ì´ˆê¸° ë©”ì‹œì§€ë¥¼ ì¼ë°˜ì ì¸ ë‚´ìš©ìœ¼ë¡œ ë³€ê²½
    if not chat_history.messages:
        chat_history.add_ai_message("ì•ˆë…•í•˜ì„¸ìš”! `data` í´ë”ì˜ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ“š")

    for msg in chat_history.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
        st.chat_message("human").write(prompt)

        with st.chat_message("ai"):
            with st.spinner("ğŸ§ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    config = {"configurable": {"session_id": "rag_chat"}}
                    response = conversational_rag_chain.invoke({"input": prompt}, config)
                    
                    formatted_response = format_output(response)
                    st.write(formatted_response['answer'])

                    with st.expander(f"ğŸ“„ ì°¸ê³  ë¬¸ì„œ ({formatted_response['source_count']}ê°œ)"):
                        if formatted_response['context']:
                            for i, doc in enumerate(formatted_response['context']):
                                st.markdown(f"**ğŸ“– ë¬¸ì„œ {i+1}**")
                                source = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                                st.markdown(f"**ì¶œì²˜:** `{source}`")
                                
                                # í˜ì´ì§€ ë²ˆí˜¸ëŠ” PDF íŒŒì¼ì—ë§Œ ì¡´ì¬í•  ìˆ˜ ìˆìŒ
                                if 'page' in doc.metadata:
                                    page = doc.metadata.get('page')
                                    st.markdown(f"**í˜ì´ì§€:** {page + 1}")

                                st.text_area(
                                    f"ë¬¸ì„œ {i+1} ë‚´ìš©",
                                    doc.page_content,
                                    height=150,
                                    key=f"doc_{i}",
                                    label_visibility="collapsed"
                                )
                                if i < len(formatted_response['context']) - 1:
                                    st.markdown("---")
                        else:
                            st.info("ë‹µë³€ì— ì°¸ê³ í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    st.info("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
