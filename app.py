import os
import time
import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = os.getenv("MY_OPENAI_API_KEY")

#cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_documents(_docs)

    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
    vectorstore = FAISS.from_documents(split_docs, embeddings)

    # ë¡œì»¬ì— ì €ì¥ (ì˜µì…˜: ë‚˜ì¤‘ì— get_vectorstoreì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    vectorstore.save_local("faiss_index")
    return vectorstore

# FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œì»¬ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
@st.cache_resource
def get_vectorstore(_docs):
    if os.path.exists("faiss_index/index.faiss") and os.path.exists("faiss_index/index.pkl"):
        embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
        return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    else:
        return create_vector_store(_docs)


# data í´ë”ì˜ PDF ì „ë¶€ ë¡œë“œí•´ì„œ ë²¡í„° DB ë§Œë“¤ê³ , ê²€ìƒ‰ + íˆìŠ¤í† ë¦¬ê¹Œì§€ í¬í•¨í•œ ì „ì²´ Chain êµ¬ì„±
@st.cache_resource
def initialize_components(selected_model):
    data_dir = "./data"
    all_pages = []

    for filename in os.listdir(data_dir):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(data_dir, filename)
            try:
                pages = load_and_split_pdf(file_path)
                all_pages.extend(pages)
            except Exception as e:
                st.warning(f"âš ï¸ {filename} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    if not all_pages:
        st.error("âŒ data í´ë”ì— ìœ íš¨í•œ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° retriever ì¶”ì¶œ
    vectorstore = get_vectorstore(all_pages)
    retriever = vectorstore.as_retriever()

    # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ retrieverë¥¼ ìœ„í•œ system prompt
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ,
        ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""


    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    qa_system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì¹˜ë§¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë…¸ì¸ì„ ìœ„í•œ êµ­ê°€ ì§€ì› ì œë„, ë³µì§€ í˜œíƒ, ì˜ë£Œ ë° ëŒë´„ ì •ë³´ ë“± ì¹˜ë§¤ ì „ë°˜ì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìµí•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  
    ë‹µë³€ì€ ê°„ê²°í•˜ê³  **ìµœëŒ€í•œ 8ì¤„ ì´ë‚´ë¡œ ì„¤ëª…**í•´ì£¼ê³ , ì–´ë¦°ì´ë„ ì´í•´í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

    ---  
    **ë‹µë³€ ê¸°ì¤€ ë° ê·œì¹™**  

    1. ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œ(context)ê°€ ì¡´ì¬í•  ê²½ìš°
        - ë°˜ë“œì‹œ **context ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ** ë‹µë³€í•˜ì„¸ìš”.  
        - ì¼ë°˜ì ì¸ ë°°ê²½ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ì¶œì²˜ë‚˜ ìª½ìˆ˜ëŠ” **í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.**
        - ì—°ë½ì²˜ë¥¼ ë¬¼ì–´ë³´ëŠ”ê²½ìš° ì—°ë½ì²˜ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.  
        

    2. ì•„ë˜ contextê°€ ë¹„ì–´ ìˆì„ ê²½ìš°
        - GPT ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ë§Œì„ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”.  
        - ì´ ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒ ë¬¸ì¥ì„ **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

    3. ë¬¸ì„œì—ì„œ ì œê³µë˜ì§€ ì•ŠëŠ” ì •ë³´ì˜ ê²½ìš°
        - **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

         ---  
         ì°¸ê³  ë¬¸ì„œ (context):  
         {context}

     """


    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # LLM, íˆìŠ¤í† ë¦¬ í¬í•¨ retriever, QA chain êµ¬ì„±
    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # ìµœì¢… RAG chain ë°˜í™˜
    return rag_chain


# Streamlit UI
st.set_page_config(page_title="ë©”ëª¨ë¦¬ë„¤ë¹„ ğŸ’¬ğŸ“š", page_icon="ğŸ§­", layout="centered")

st.markdown("""
<div class="title-section">
    <h1>ğŸ§­ ë©”ëª¨ë¦¬ë„¤ë¹„</h1>
    <p>ì–´ë¥´ì‹ ì„ ìœ„í•œ ì¹˜ë§¤ ê´€ë ¨ ì •ë³´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.<br>
    ê¶ê¸ˆí•œ ë‚´ìš©ì„ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!</p>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<style>
/* ì „ì²´ ì±„íŒ… ë©”ì‹œì§€ ê¸°ë³¸ ê¸€ì”¨ í¬ê¸° */
.stChatMessage {
    font-size: 48px;
    line-height: 1.6;
}

/* ì‚¬ìš©ì ì…ë ¥ì°½ ê¸€ì í¬ê¸° */
.stTextInput > div > input {
    font-size: 24px !important;
}

/* íƒ€ì´í‹€ ì˜ì—­ ìŠ¤íƒ€ì¼ */
.title-section h1 {
    color: #000000 !important;
}
.title-section p {
    color: #555555 !important;
}
.title-section {
    text-align: center;
    background-color: #f8f9f9;
    padding: 1.2rem;
    border-radius: 12px;
    margin-bottom: 1.2rem;
    border: 1px solid #ddd;
}

/* AI ë‹µë³€ í…ìŠ¤íŠ¸ í¬ê¸° (ê¸°ë³¸) */
.stChatMessage p {
    font-size: 24px !important;    
    line-height: 1.6 !important;
}

/* ì²« ë²ˆì§¸ AI ë²„ë¸” ì—¬ë°± ë° ë†’ì´ */
div[data-testid="stChatMessage"][data-variant="assistant"]:first-of-type {
    padding: 1.2rem 1.5rem !important;
    min-height: 96px !important;
}

/* ì…ë ¥ì°½ ìì²´ ë†’ì´ ë° ì •ë ¬ */
[data-testid="stChatInput"] > div:first-child {
    min-height: 64px !important;
    display: flex;
    align-items: center;
    padding: 0 1rem !important;
}

/* ì…ë ¥ì°½ í…ìŠ¤íŠ¸ ì˜ì—­ */
[data-testid="stChatInput"] textarea {
    font-size: 20px !important;
    line-height: 1.6 !important;
    padding: 0.6rem 0.5rem !important;
}

/* ì…ë ¥ì°½ placeholder */
[data-testid="stChatInput"] textarea::placeholder {
    font-size: 20px !important;
    opacity: 0.7;
}

/* AI ë©”ì‹œì§€ ì „ì²´ í­ ë„“íˆê¸° */
div[data-testid="stChatMessage"][data-variant="assistant"] {
    padding-left: 2rem !important;
    padding-right: 2rem !important;
    max-width: 100% !important;
}

/* <b>, <strong>, <li> ë“± ê°•ì¡° í…ìŠ¤íŠ¸ë„ ê¸€ì í¬ê²Œ */
.stMarkdown b,
.stMarkdown strong,
.stMarkdown li,
.stMarkdown p {
    font-size: 24px !important;
    line-height: 1.6 !important;
}
</style>
""", unsafe_allow_html=True)




# chat_history = StreamlitChatMessageHistory(key="chat_messages")


# option = st.selectbox("ì‚¬ìš©í•˜ì‹¤ GPT ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”. (ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤)", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))
selected_model = "gpt-4o-mini"
rag_chain = initialize_components(selected_model)
chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë¬¸ë§¥/ì±„íŒ… ë©”ì‹œì§€)
if "context" not in st.session_state:
    st.session_state["context"] = []

# ì±— íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´, ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ ì¶”ê°€
# if not chat_history.messages:
#     chat_history.add_ai_message("ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥
for msg in chat_history.messages:
    # ì±— ë©”ì‹œì§€ ì¶œë ¥ ì‹œ í¬ê¸° ì¡°ì ˆ
    if msg.type == "human":
        # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ëŠ” íŒŒë€ìƒ‰ìœ¼ë¡œ, í°íŠ¸ í¬ê¸° 24px
        st.chat_message("human").markdown(f"<span style='font-size:24px; color:#007BFF;'>{msg.content}</span>", unsafe_allow_html=True)
    else: # msg.type == "ai"
        # AIì˜ ë©”ì‹œì§€ëŠ” ê¸°ë³¸ ìƒ‰ìƒìœ¼ë¡œ, í°íŠ¸ í¬ê¸° 24px
        st.chat_message("ai").markdown(f"<span style='font-size:24px;'>{msg.content}</span>", unsafe_allow_html=True)

    
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ëŠ” ë¶€ë¶„
if prompt_message := st.chat_input("ì¹˜ë§¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì—¬ê¸°ì— ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    if conversational_rag_chain: # ì±—ë´‡ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ ì²˜ë¦¬
        # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì¶œë ¥ (ê¸€ì”¨ í¬ê¸° ë° ìƒ‰ìƒ ì¡°ì ˆ)
        st.chat_message("human").markdown(f"<span style='font-size:24px; color:#007BFF;'>{prompt_message}</span>", unsafe_allow_html=True)

        with st.chat_message("ai"):
            with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤... ğŸ§"):
                config = {"configurable": {"session_id": "any"}}
                response = conversational_rag_chain.invoke(
                    {"input": prompt_message},
                    config
                )

                answer = response['answer']
                # AIì˜ ë‹µë³€ ì¶œë ¥ (ê¸€ì”¨ í¬ê¸° ì¡°ì ˆ)
                placeholder = st.empty()
                for i in range(len(answer)):
                    placeholder.markdown(f"<span style='font-size:24px;'>{answer[:i+1]}</span>", unsafe_allow_html=True)
                    time.sleep(0.01)  # íƒ€ì´í•‘ ì†ë„ ì¡°ì ˆ (ë¹ ë¥´ê²Œ í•˜ê³  ì‹¶ìœ¼ë©´ ì¤„ì´ê¸°)

            # ì°¸ê³  ë¬¸ì„œ ìœ ì‚¬ë„ í•„í„°ë§ ë° ì¶œë ¥ (ìœ ì‚¬ë„ 0.3 ì´ìƒë§Œ)
            vectorstore = get_vectorstore([])  # ê¸°ì¡´ vectorstore ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
            scored_docs = vectorstore.similarity_search_with_score(prompt_message, k=2)

            filtered_docs = []
            for doc, score in scored_docs:
                sim_score = 1 - score / 2  # FAISS (cosine) ê¸°ì¤€ ë³€í™˜
                if sim_score >= 0.4:
                    filtered_docs.append(doc)  # ì ìˆ˜ëŠ” ì•ˆ ì”€

            # ë²„íŠ¼(Expander) ëˆŒë €ì„ ë•Œë§Œ í‘œì‹œ
            if filtered_docs:
                with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    for doc in filtered_docs:
                        source = os.path.basename(doc.metadata.get("source", ""))
                        page = doc.metadata.get("page", None)
                        if source and page is not None:
                            st.markdown(f"- ğŸ“„ {source} - {page + 1}ìª½")
                        else:
                            st.markdown("- â” ì¶œì²˜ ì—†ìŒ")

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (AI ë²„ë¸”ë¡œ ë„£ê¸°)
if not chat_history.messages:
    st.markdown("""
    <div style='text-align: center; margin-top: 1.5rem;'>
        <h2 style='margin: 0; font-size: 28px;'>ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!</h2>
    </div>
    """, unsafe_allow_html=True)